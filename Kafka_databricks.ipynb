{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fb89ec9-2c39-4369-a6a4-5aef0b18516d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e19f02-857d-48a5-bba3-9a801cbe2ccb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pyspark functions\n",
    "from pyspark.sql.functions import *\n",
    "# URL processing\n",
    "import urllib\n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c272abd6-7488-4a24-a399-a801e78c4714",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pyspark functions\n",
    "from pyspark.sql.functions import *\n",
    "# URL processing\n",
    "import urllib\n",
    "\n",
    "# AWS S3 bucket name\n",
    "AWS_S3_BUCKET = \"user-12ffc5aba733-bucket\"\n",
    "# Mount name for the bucket\n",
    "MOUNT_NAME = \"/mnt/12ffc5aba733\"\n",
    "# Source url\n",
    "SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)\n",
    "# Mount the drive\n",
    "\n",
    "try:\n",
    "  dbutils.fs.unmount(SOURCE_URL, MOUNT_NAME)\n",
    "except:\n",
    "  pass\n",
    "\n",
    "try:\n",
    "  dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "416b0d5c-5dd2-4b67-a656-fb80a7538678",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0df7e47-d912-4b4f-8c25-2b2c28f7de89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.fs.ls('/mnt/12ffc5aba733/topics/12ffc5aba733.pin/partition=0/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e055179-07c4-4334-b707-ddf4bab63649",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "# Asterisk(*) indicates reading all the content of the specified file that have .json extension\n",
    "file_location = \"/mnt/12ffc5aba733/topics/12ffc5aba733.pin/partition=0/*.json\" \n",
    "file_type = \"json\"\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket\n",
    "df_pin = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "# Display Spark dataframe to check its content\n",
    "display(df_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cfbce72-936b-4c68-81dc-bd8be70588a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "# Asterisk(*) indicates reading all the content of the specified file that have .json extension\n",
    "file_location = \"/mnt/12ffc5aba733/topics/12ffc5aba733.geo/partition=0/*.json\" \n",
    "file_type = \"json\"\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket\n",
    "df_geo = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "# Display Spark dataframe to check its content\n",
    "display(df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f034ef6b-5be8-41a9-9795-4a5b53714e74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "# Asterisk(*) indicates reading all the content of the specified file that have .json extension\n",
    "file_location = \"/mnt/12ffc5aba733/topics/12ffc5aba733.user/partition=0/*.json\" \n",
    "file_type = \"json\"\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket\n",
    "df_user = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "# Display Spark dataframe to check its content\n",
    "display(df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5840518f-f92c-4a4d-b920-c1fef92896f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "loaded_df = spark.table(\"df_geo\")\n",
    "display(loaded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c924e55-f547-4019-afb8-327c73f1bfc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a87ca04b-9977-476b-bda4-2f35bdbb7214",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Replace empty entries or invalid data with None\n",
    "df_pin_cleaned = df_pin.replace([\"\", \"N/A\",\"User Info Error\", \"No description available Story format\", \"Image src error\",\"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\", \"No Title Data Available\",], None)\n",
    "# Clean and cast `follower_count` to IntegerType\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"follower_count\", regexp_replace(col(\"follower_count\"), \" \", \"\"))\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"follower_count\",\n",
    "    when(col(\"follower_count\").contains(\"k\"), \n",
    "         regexp_replace(col(\"follower_count\"), \"k\", \"\").cast(\"float\") * 1000)\n",
    "    .when(col(\"follower_count\").contains(\"m\"), \n",
    "         regexp_replace(col(\"follower_count\"), \"m\", \"\").cast(\"float\") * 1000000)\n",
    "    .when(col(\"follower_count\").contains(\"M\"), \n",
    "         regexp_replace(col(\"follower_count\"), \"M\", \"\").cast(\"float\") * 1000000)\n",
    "    .otherwise(col(\"follower_count\").cast(\"float\"))\n",
    ")\n",
    "\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"follower_count\", col(\"follower_count\").cast(\"int\"))\n",
    "\n",
    "#General cleaning on other columns\n",
    "\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"downloaded\", col(\"downloaded\").cast(IntegerType()))\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"index\", col(\"index\").cast(IntegerType()))\n",
    "\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"save_location\", split(col(\"save_location\"), \" \").getItem(3))\n",
    "\n",
    "#Rename columns\n",
    "                \n",
    "df_pin_cleaned = df_pin_cleaned.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "# Reorder the DataFrame columns \n",
    "\n",
    "df_pin_cleaned = df_pin_cleaned.select(\"ind\", \"unique_id\", \"title\", \"description\", \n",
    "                                       \"follower_count\", \"poster_name\", \"tag_list\", \n",
    "                                       \"is_image_or_video\", \"image_src\", \"save_location\", \"category\")\n",
    "display(df_pin_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d2be4e-193f-43b2-bb80-d70e4d07bd2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create an array between lat and long\n",
    "\n",
    "df_geo_cleaned = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "\n",
    "# Drop columns from the DataFrame.\n",
    "df_geo_cleaned = df_geo_cleaned.drop(\"latitude\").drop(\"longitude\")\n",
    "\n",
    "# Convert the timestamp column from a string to a timestamp data type.\n",
    "df_geo_cleaned = df_geo_cleaned.withColumn(\"timestamp\", col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Reorder the DataFrame columns \n",
    "df_geo_cleaned = df_geo_cleaned.select(\"ind\", \"country\", \"coordinates\", \"timestamp\")\n",
    "\n",
    "display(df_geo_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e16e9f2-e0b4-4e9e-aa4b-583a353b4332",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Concat both first name and last name via a space. \n",
    "\n",
    "df_user_cleaned = df_user.withColumn(\"user_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "\n",
    "# Drop the columns from the DataFrame.\n",
    "df_user_cleaned = df_user_cleaned.drop(\"first_name\").drop(\"last_name\")\n",
    "\n",
    "# Convert the column from a string to a timestamp data type.\n",
    "df_user_cleaned = df_user_cleaned.withColumn(\"date_joined\", col(\"date_joined\").cast(TimestampType()))\n",
    "\n",
    "#  Reorder the DataFrame columns \n",
    "df_user_cleaned = df_user_cleaned.select(\"ind\", \"user_name\", \"age\", \"date_joined\")\n",
    "\n",
    "display(df_user_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d88fada9-c8a4-4481-974b-89edf33d2d47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Task 4\n",
    "\n",
    "df_combined = df_pin_cleaned.join(df_geo_cleaned, df_geo_cleaned[\"ind\"] == df_pin_cleaned[\"ind\"], how=\"inner\")\n",
    "\n",
    "#Group by country and category, and count the occurrences of each category in each country\n",
    "\n",
    "category_count_df = df_combined.groupBy(\"country\", \"category\").agg(count(\"*\").alias(\"category_count\"))\n",
    "\n",
    "sorted_df = category_count_df.orderBy(\"category_count\", ascending=False)\n",
    "\n",
    "display(sorted_df )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0e84905-dcd6-4c55-a8c0-7ab66351ff0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Task 5 \n",
    "\n",
    "#Join on both df\n",
    "\n",
    "df_combined_2 = df_pin_cleaned.join(df_geo_cleaned, on=\"ind\")\n",
    "\n",
    "#Group by timestamp and catergory where there is matching\n",
    "\n",
    "category_count_df_2 = df_combined_2.groupBy(year(\"timestamp\").alias(\"post_year\"), \"category\") \\\n",
    "                               .agg(count(\"*\").alias(\"category_count\"))\n",
    "\n",
    "# Filter years between 2018 and 2022\n",
    "filtered_df_2 = category_count_df_2.where(col(\"post_year\").between(2018, 2022))\n",
    "\n",
    "# Sort the DataFrame by category_count in descending order\n",
    "sorted_df_2 = filtered_df_2.orderBy(\"category_count\", ascending=False)\n",
    "\n",
    "display(sorted_df_2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28eb70aa-5261-4d60-ad59-892f4c4b6603",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Task 6\n",
    "\n",
    "df_combined_3 = df_pin_cleaned.join(df_geo_cleaned, df_geo_cleaned[\"ind\"] == df_pin_cleaned[\"ind\"], how=\"inner\")\n",
    "\n",
    "df_country_max_followers = df_combined_3.groupBy(\"country\", \"poster_name\") \\\n",
    "            .agg(max(\"follower_count\").alias(\"follower_count\"))\n",
    "\n",
    "\n",
    "sorted_df_3 = df_country_max_followers.orderBy(\"follower_count\", ascending=False)\n",
    "\n",
    "\n",
    "#display(sorted_df_3)\n",
    "\n",
    "\n",
    "#part 2\n",
    "\n",
    "df_max_follower_country = df_country_max_followers.orderBy(col(\"follower_count\").desc()).limit(1).select(\"country\", \"follower_count\")\n",
    "\n",
    "display(df_max_follower_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb0c3d11-9783-4410-80a9-bdf85bcd96fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Task 7\n",
    "df_combined_4 = df_pin_cleaned.join(df_user_cleaned, df_user_cleaned[\"ind\"] == df_pin_cleaned[\"ind\"], how=\"inner\")\n",
    "\n",
    "df_age_grouped = df_combined_4.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\").between(18, 24), \"18-24\")\n",
    "    .when(col(\"age\").between(25, 35), \"25-35\")\n",
    "    .when(col(\"age\").between(36, 50), \"36-50\")\n",
    "    .when(col(\"age\") > 50, \"50+\")\n",
    ")\n",
    "\n",
    "\n",
    "df_ages = df_age_grouped.groupBy(\"age_group\", \"category\") \\\n",
    "            .agg(count(\"*\").alias(\"category_count\"))\n",
    "\n",
    "sorted_df_4 = df_ages.orderBy(\"category_count\", ascending=False)\n",
    "\n",
    "display(sorted_df_4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7677933f-13bd-4a87-9bd1-380effd7ba89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Task 8\n",
    "df_combined_5 = df_pin_cleaned.join(df_user_cleaned, df_user_cleaned[\"ind\"] == df_pin_cleaned[\"ind\"], how=\"inner\")\n",
    "\n",
    "df_age_grouped = df_combined_5.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\").between(18, 24), \"18-24\")\n",
    "    .when(col(\"age\").between(25, 35), \"25-35\")\n",
    "    .when(col(\"age\").between(36, 50), \"36-50\")\n",
    "    .when(col(\"age\") > 50, \"50+\")\n",
    ")\n",
    "\n",
    "df_ages_2 = df_age_grouped.groupBy(\"age_group\") \\\n",
    "            .agg(percentile_approx(\"follower_count\", 0.5).alias(\"median_follower_count\"))\n",
    "\n",
    "sorted_df_5 = df_ages_2.orderBy(\"median_follower_count\", ascending=False)            \n",
    "\n",
    "\n",
    "display(sorted_df_5)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d706c045-1e82-49d1-a90b-f71b7f4b1147",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Task 9\n",
    "\n",
    "\n",
    "df_age_grouped_2 = df_user_cleaned.withColumn(\n",
    "    \"year_joined\",\n",
    "    when(year(\"date_joined\") == 2015, \"2015\")\n",
    "    .when(year(\"date_joined\") == 2016, \"2016\")\n",
    "    .when(year(\"date_joined\") == 2017, \"2017\")\n",
    "    .when(year(\"date_joined\") == 2018, \"2018\")\n",
    "    .when(year(\"date_joined\") == 2019, \"2019\")\n",
    "    .when(year(\"date_joined\") == 2020, \"2020\")\n",
    "    \n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "users_joined_df = df_age_grouped_2.groupBy(year(\"year_joined\").alias(\"post_year\")) \\\n",
    "                               .agg(count(\"*\").alias(\"number_users_joined\"))\n",
    "\n",
    "\n",
    "sorted_df_6 = users_joined_df.orderBy(\"number_users_joined\", ascending=False)         \n",
    "\n",
    "\n",
    "display(sorted_df_6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d3dbddb-bdd0-470d-a287-6b85db58d0b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df_with_year = df_user_cleaned.withColumn(\"post_year\", year(\"date_joined\"))\n",
    "\n",
    "df_filtered = df_with_year.filter(col(\"post_year\").between(2015, 2020))\n",
    "\n",
    "users_joined_df = df_filtered.groupBy(\"post_year\").agg(count(\"*\").alias(\"number_users_joined\"))\n",
    "\n",
    "sorted_df = users_joined_df.orderBy(\"post_year\", ascending=True)\n",
    "\n",
    "sorted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "158715b6-53e8-4dfb-879d-3e03d5dce47b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#task 10\n",
    "\n",
    "df_combined_6 = df_pin_cleaned.join(df_user_cleaned, df_user_cleaned[\"ind\"] == df_pin_cleaned[\"ind\"], how=\"inner\")\n",
    "\n",
    "df_with_year = df_combined_6.withColumn(\"post_year\", year(\"date_joined\"))\n",
    "\n",
    "df_filtered = df_with_year.filter(col(\"post_year\").between(2015, 2020))\n",
    "\n",
    "users_joined_df = df_filtered.groupBy(\"post_year\").agg(percentile_approx(\"follower_count\", 0.5).alias(\"median_follower_count\"))\n",
    "\n",
    "sorted_df_7 = users_joined_df.orderBy(\"median_follower_count\", ascending=False)    \n",
    "\n",
    "sorted_df_7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53369310-0669-475b-bf79-9faffd14ace6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#task 11\n",
    "\n",
    "df_combined_6 = df_pin_cleaned.join(df_user_cleaned, df_user_cleaned[\"ind\"] == df_pin_cleaned[\"ind\"], how=\"inner\")\n",
    "\n",
    "df_with_year = df_combined_6.withColumn(\"post_year\", year(\"date_joined\"))\n",
    "\n",
    "df_filtered = df_with_year.filter(col(\"post_year\").between(2015, 2020))\n",
    "\n",
    "df_age_year = df_filtered.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\").between(18, 24), \"18-24\")\n",
    "    .when(col(\"age\").between(25, 35), \"25-35\")\n",
    "    .when(col(\"age\").between(36, 50), \"36-50\")\n",
    "    .when(col(\"age\") > 50, \"50+\")\n",
    ")\n",
    "\n",
    "df_grouped_final = df_age_year.groupBy(\"post_year\", \"age_group\").agg(percentile_approx(\"follower_count\", 0.5).alias(\"median_follower_count\"))\n",
    "\n",
    "sorted_df_8 = df_grouped_final.orderBy(\"post_year\", ascending=False)    \n",
    "\n",
    "sorted_df_8.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1445726648212721,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2024-08-22 15:52:33",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
