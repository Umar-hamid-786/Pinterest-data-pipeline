{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5951213c-bd45-4e50-a325-9c276f9682c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59d26fc6-0aee-4dca-b8b0-f17e7c9a3181",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pyspark functions\n",
    "from pyspark.sql.functions import *\n",
    "# URL processing\n",
    "import urllib\n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a98f4439-5779-4c9e-9ea5-ae9c5ec20972",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Disable format checks during the reading of Delta tables\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436b4e28-9934-4cd1-a090-b16e56206c7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "#Load the kinesis data from stream to databricks\n",
    "\n",
    "df_pin = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-12ffc5aba733-pin') \\\n",
    ".option('initialPosition','latest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "df_pin = df_pin.selectExpr(\"CAST(data as STRING)\")\n",
    "\n",
    "#Creation of schema to convert and parse.\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"index\", StringType(), True),\n",
    "    StructField(\"unique_id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"poster_name\", StringType(), True),\n",
    "    StructField(\"follower_count\", StringType(), True),  \n",
    "    StructField(\"tag_list\", StringType(), True),\n",
    "    StructField(\"is_image_or_video\", StringType(), True),\n",
    "    StructField(\"image_src\", StringType(), True),\n",
    "    StructField(\"downloaded\", LongType(), True),  \n",
    "    StructField(\"save_location\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "#Parse data into a table from a JSON string via the schema. \n",
    "\n",
    "df_pin = df_pin.withColumn(\"jsonData\", from_json(col(\"data\").cast(\"string\"), schema))\n",
    "\n",
    "#Rename columns\n",
    "\n",
    "df_pin_kinesis = df_pin.select(\n",
    "    col(\"jsonData.index\").alias(\"index\"),\n",
    "    col(\"jsonData.unique_id\").alias(\"unique_id\"),\n",
    "    col(\"jsonData.title\").alias(\"title\"),\n",
    "    col(\"jsonData.description\").alias(\"description\"),\n",
    "    col(\"jsonData.poster_name\").alias(\"poster_name\"),\n",
    "    col(\"jsonData.follower_count\").alias(\"follower_count\"),\n",
    "    col(\"jsonData.tag_list\").alias(\"tag_list\"),\n",
    "    col(\"jsonData.is_image_or_video\").alias(\"is_image_or_video\"),\n",
    "    col(\"jsonData.image_src\").alias(\"image_src\"),\n",
    "    col(\"jsonData.downloaded\").alias(\"downloaded\"),\n",
    "    col(\"jsonData.save_location\").alias(\"save_location\"),\n",
    "    col(\"jsonData.category\").alias(\"category\")\n",
    ")\n",
    "\n",
    "# Replace empty entries or invalid data with None\n",
    "\n",
    "df_pin_cleaned = df_pin_kinesis.replace([\"\", \"N/A\",\"User Info Error\", \"No description available Story format\", \"Image src error\",\"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\", \"No Title Data Available\",], None)\n",
    "\n",
    "# Clean and cast to IntegerType\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"follower_count\", regexp_replace(col(\"follower_count\"), \" \", \"\"))\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"follower_count\",\n",
    "    when(col(\"follower_count\").contains(\"k\"), \n",
    "         regexp_replace(col(\"follower_count\"), \"k\", \"\").cast(\"float\") * 1000)\n",
    "    .when(col(\"follower_count\").contains(\"m\"), \n",
    "         regexp_replace(col(\"follower_count\"), \"m\", \"\").cast(\"float\") * 1000000)\n",
    "    .when(col(\"follower_count\").contains(\"M\"), \n",
    "         regexp_replace(col(\"follower_count\"), \"M\", \"\").cast(\"float\") * 1000000)\n",
    "    .otherwise(col(\"follower_count\").cast(\"float\"))\n",
    ")\n",
    "\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"follower_count\", col(\"follower_count\").cast(\"int\"))\n",
    "\n",
    "\n",
    "# More cleaning\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"downloaded\", col(\"downloaded\").cast(IntegerType()))\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"index\", col(\"index\").cast(IntegerType()))\n",
    "\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"save_location\", split(col(\"save_location\"), \" \").getItem(3))\n",
    "                \n",
    "df_pin_cleaned = df_pin_cleaned.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "df_pin_cleaned = df_pin_cleaned.select(\"ind\", \"unique_id\", \"title\", \"description\", \n",
    "                                       \"follower_count\", \"poster_name\", \"tag_list\", \n",
    "                                       \"is_image_or_video\", \"image_src\", \"save_location\", \"category\")\n",
    "display(df_pin_cleaned)\n",
    "\n",
    "#Write to delta table\n",
    "\n",
    "df_pin_cleaned.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"12ffc5aba733_pin_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5cb0d14-0ef8-4509-8939-949b4078d7d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "spark.sql(\"TRUNCATE TABLE default.12ffc5aba733_user_table\")\n",
    "\n",
    "df_user = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-12ffc5aba733-user') \\\n",
    ".option('initialPosition','latest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "df_user = df_user.selectExpr(\"CAST(data as STRING)\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ind\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", StringType(), True),\n",
    "    StructField(\"date_joined\", StringType(), True),\n",
    "])\n",
    "\n",
    "df_user = df_user.withColumn(\"jsonData\", from_json(col(\"data\").cast(\"string\"), schema))\n",
    "\n",
    "df_user_kinesis = df_user.select(\n",
    "    col(\"jsonData.ind\").alias(\"ind\"),\n",
    "    col(\"jsonData.first_name\").alias(\"first_name\"),\n",
    "    col(\"jsonData.last_name\").alias(\"last_name\"),\n",
    "    col(\"jsonData.age\").alias(\"age\"),\n",
    "    col(\"jsonData.date_joined\").alias(\"date_joined\"),\n",
    ")\n",
    "\n",
    "df_user_cleaned = df_user_kinesis.withColumn(\"user_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "\n",
    "# Drop the columns from the DataFrame.\n",
    "df_user_cleaned = df_user_cleaned.drop(\"first_name\").drop(\"last_name\")\n",
    "\n",
    "# Convert the column from a string to a timestamp data type.\n",
    "df_user_cleaned = df_user_cleaned.withColumn(\"date_joined\", col(\"date_joined\").cast(TimestampType()))\n",
    "\n",
    "#  Reorder the DataFrame columns \n",
    "df_user_cleaned = df_user_cleaned.select(\"ind\", \"user_name\", \"age\", \"date_joined\")\n",
    "\n",
    "display(df_user_cleaned)\n",
    "\n",
    "#Write to delta table\n",
    "\n",
    "df_user_cleaned.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"12ffc5aba733_user_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f28bc5-3c53-4119-9af3-aa922b259b1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark.sql(\"TRUNCATE TABLE default.12ffc5aba733_geo_table\")\n",
    "\n",
    "df_geo = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-12ffc5aba733-geo') \\\n",
    ".option('initialPosition','latest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "df_geo  = df_geo.selectExpr(\"CAST(data as STRING)\")\n",
    "\n",
    "\n",
    "schema_geo = StructType([\n",
    "    StructField(\"ind\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"latitude\", StringType(), True),\n",
    "    StructField(\"longitude\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "])\n",
    "\n",
    "df_geo = df_geo.withColumn(\"jsonData\", from_json(col(\"data\").cast(\"string\"), schema_geo))\n",
    "\n",
    "df_geo_kinesis = df_geo.select(\n",
    "    col(\"jsonData.ind\").alias(\"ind\"),\n",
    "    col(\"jsonData.timestamp\").alias(\"timestamp\"),\n",
    "    col(\"jsonData.latitude\").alias(\"latitude\"),\n",
    "    col(\"jsonData.longitude\").alias(\"longitude\"),\n",
    "    col(\"jsonData.country\").alias(\"country\"),\n",
    ")\n",
    "\n",
    "df_geo_cleaned = df_geo_kinesis.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "# Drop the columns from the DataFrame.\n",
    "df_geo_cleaned = df_geo_cleaned.drop(\"latitude\").drop(\"longitude\")\n",
    "\n",
    "# Convert the column from a string to a timestamp data type.\n",
    "df_geo_cleaned = df_geo_cleaned.withColumn(\"timestamp\", col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Reorder the DataFrame columns \n",
    "df_geo_cleaned = df_geo_cleaned.select(\"ind\", \"country\", \"coordinates\", \"timestamp\")\n",
    "\n",
    "display(df_geo_cleaned)\n",
    "\n",
    "# Write the streaming data to a file sink\n",
    "df_geo_cleaned.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"12ffc5aba733_geo_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a61392d4-3bb5-4c36-ba04-81960f55d7d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/\", True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 552522677389923,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2024-09-04 18:48:42",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
